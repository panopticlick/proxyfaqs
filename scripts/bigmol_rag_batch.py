#!/usr/bin/env python3
"""
BigModel æ‰¹é‡æ–‡ç« ç”Ÿæˆå™¨ + RAG
- ä½¿ç”¨çŸ¥è¯†åº“ (knowledge_base.jsonl) è¿›è¡Œ RAG æ£€ç´¢
- å¹¶å‘ç”Ÿæˆ
- è‡ªåŠ¨å¤‡ä»½
- é”™è¯¯é‡è¯•
"""
import os
import json
import time
import subprocess
import shutil
import re
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learn not available, RAG disabled")

ARTICLES_DIR = Path("output/articles")
QUESTIONS_FILE = "output/batch86_questions.jsonl"
KNOWLEDGE_BASE = "output/knowledge_base.jsonl"
BIGMODEL_BIN = os.path.expanduser("~/.claude/bin/bigmodel_router.sh")
BACKUP_DIR = Path(f"output/backups/{datetime.now().strftime('%Y%m%d_%H%M%S')}")

MAX_PARALLEL = 2  # BigModel å¹¶å‘é™åˆ¶
MAX_RETRIES = 2
TOP_K = 12  # RAG æ£€ç´¢æ•°é‡


class RAGRetriever:
    """RAG çŸ¥è¯†æ£€ç´¢å™¨"""

    def __init__(self, knowledge_base_path: str, top_k: int = TOP_K):
        self.knowledge_base_path = Path(knowledge_base_path)
        self.top_k = top_k
        self.knowledge_entries: List[Dict] = []
        self.vectorizer = None
        self.tfidf_matrix = None
        self.loaded = False

    def load(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        if not self.knowledge_base_path.exists():
            print(f"âš ï¸ çŸ¥è¯†åº“ä¸å­˜åœ¨: {self.knowledge_base_path}")
            return False

        if not SKLEARN_AVAILABLE:
            print("âš ï¸ scikit-learn æœªå®‰è£…ï¼Œè·³è¿‡ RAG")
            return False

        print(f"åŠ è½½çŸ¥è¯†åº“: {self.knowledge_base_path}")

        with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    if entry.get('text'):
                        self.knowledge_entries.append(entry)
                except:
                    pass

        if len(self.knowledge_entries) == 0:
            print("âš ï¸ çŸ¥è¯†åº“ä¸ºç©º")
            return False

        print(f"  âœ“ åŠ è½½ {len(self.knowledge_entries):,} æ¡çŸ¥è¯†")

        # æ„å»º TF-IDF ç´¢å¼•
        print("  æ„å»º RAG ç´¢å¼•...")
        texts = [entry.get('text', '') for entry in self.knowledge_entries]

        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.85
        )

        self.tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"  âœ“ ç´¢å¼•å½¢çŠ¶: {self.tfidf_matrix.shape}")
        self.loaded = True
        return True

    def retrieve(self, question: str, top_k: int = None) -> str:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not self.loaded:
            return ""

        if top_k is None:
            top_k = self.top_k

        # å‘é‡åŒ–é—®é¢˜
        q_vec = self.vectorizer.transform([question])
        similarities = cosine_similarity(q_vec, self.tfidf_matrix).flatten()

        # è·å– top-k
        top_indices = similarities.argsort()[-top_k:][::-1]

        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context_parts = []
        for idx in top_indices:
            if similarities[idx] > 0.03:
                text = self.knowledge_entries[idx].get('text', '')
                text = re.sub(r'\s+', ' ', text).strip()
                if len(text) > 250:
                    text = text[:250] + "..."
                context_parts.append(f"â€¢ {text}")

        return '\n'.join(context_parts) if context_parts else ""


def setup_backup():
    """åˆ›å»ºå¤‡ä»½ç›®å½•å¹¶å¤‡ä»½ç°æœ‰æ–‡ç« """
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    if ARTICLES_DIR.exists():
        shutil.copytree(ARTICLES_DIR, BACKUP_DIR / "articles", dirs_exist_ok=True)
    print(f"âœ“ å¤‡ä»½ç›®å½•: {BACKUP_DIR}")


def get_existing_slugs() -> set:
    """è·å–å·²å®Œæˆçš„æ–‡ç«  slug"""
    existing = set()
    for f in ARTICLES_DIR.glob("*.json"):
        try:
            with open(f) as fp:
                data = json.load(fp)
                if data.get('word_count', 0) >= 1000:
                    existing.add(data.get('slug', f.stem))
        except:
            pass
    return existing


def generate_article(slug: str, question: str, volume: int, category: str,
                    context: str, retry: int = 0) -> Tuple[bool, str]:
    """ç”Ÿæˆå•ç¯‡æ–‡ç« """

    # æ„å»ºå¸¦ RAG çš„ prompt
    context_section = f"\n## å‚è€ƒçŸ¥è¯† (æ¥è‡ª Google PAA æ•°æ®)\n{context}\n" if context else ""

    prompt = f'''You are a senior proxy and web scraping expert writing for ProxyFAQs.com.

## Question
{question}

## Search Volume: {volume:,} monthly searches
## Category: {category}{context_section}

---

## REQUIREMENTS

Generate a comprehensive article with EXACTLY this structure:

### Part 1: Quick Answer (200 words)
- Direct, concise answer to the question
- Key takeaways in 2-3 sentences
- Perfect for featured snippets

### Part 2: Detailed Answer (1000+ words minimum)
- In-depth explanation with technical details
- Real-world examples and use cases
- Python code snippets when relevant
- Comparison tables where appropriate
- SEO-optimized with semantic keywords

## OUTPUT FORMAT

Return valid JSON only:
```json
{{
  "title": "SEO H1 title with main keyword [2025]",
  "meta_description": "150-160 char compelling description",
  "quick_answer": "200 word quick answer here...",
  "detailed_answer": "1000+ word detailed answer in markdown...",
  "tags": ["tag1", "tag2", "tag3"],
  "word_count": 1234
}}
```

IMPORTANT:
- Minimum total 1200 words (quick + detailed)
- Be technical and authoritative
- Include practical examples
- NO fluff - every sentence adds value
- Year should be 2025

Generate now:'''

    cmd = [BIGMODEL_BIN, prompt]
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=240)

    if result.returncode != 0:
        if retry < MAX_RETRIES:
            time.sleep(5)
            return generate_article(slug, question, volume, category, context, retry + 1)
        return False, f"Command failed: {result.stderr[:200]}"

    output = result.stdout

    # æå– JSON
    json_content = extract_json(output)
    if not json_content:
        if retry < MAX_RETRIES:
            time.sleep(5)
            return generate_article(slug, question, volume, category, context, retry + 1)
        return False, "No JSON found in output"

    # éªŒè¯å¹¶ä¿å­˜
    try:
        data = json.loads(json_content)
        data['slug'] = slug
        data['volume'] = volume

        # ç¡®ä¿ word_count å­—æ®µ
        if 'word_count' not in data:
            qa_words = len(data.get('quick_answer', '').split())
            da_words = len(data.get('detailed_answer', '').split())
            data['word_count'] = qa_words + da_words

        output_file = ARTICLES_DIR / f"{slug}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        wc = data.get('word_count', 0)
        return True, f"{wc} words"
    except Exception as e:
        if retry < MAX_RETRIES:
            time.sleep(5)
            return generate_article(slug, question, volume, category, context, retry + 1)
        return False, str(e)


def extract_json(text: str) -> str:
    """ä»è¾“å‡ºä¸­æå– JSON - æ›´å¼ºçš„è§£æ"""
    # é¦–å…ˆå°è¯•æå– ```json...``` å—
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        try:
            json.loads(json_str)  # éªŒè¯
            return json_str
        except:
            pass

    # å°è¯•æå–ä»ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } çš„å†…å®¹
    start = text.find('{')
    if start == -1:
        return ""

    # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·ï¼ˆå¤„ç†åµŒå¥—ï¼‰
    depth = 0
    for i in range(start, len(text)):
        if text[i] == '{':
            depth += 1
        elif text[i] == '}':
            depth -= 1
            if depth == 0:
                json_str = text[start:i+1]
                try:
                    json.loads(json_str)  # éªŒè¯
                    return json_str
                except:
                    break

    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªè¾“å‡º
    try:
        data = json.loads(text.strip())
        return json.dumps(data)
    except:
        pass

    return ""


def main():
    ARTICLES_DIR.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print(" BigModel æ‰¹é‡æ–‡ç« ç”Ÿæˆ + RAG")
    print("=" * 60)

    setup_backup()

    # åˆå§‹åŒ– RAG
    rag = RAGRetriever(KNOWLEDGE_BASE, top_k=TOP_K)
    rag.load()

    # è¯»å–é—®é¢˜åˆ—è¡¨
    questions = []
    with open(QUESTIONS_FILE) as fp:
        for line in fp:
            try:
                q = json.loads(line)
                questions.append(q)
            except:
                pass

    print(f"\nâœ“ å…± {len(questions)} ä¸ªé—®é¢˜")

    # è·å–å·²å®Œæˆçš„
    existing = get_existing_slugs()
    print(f"âœ“ å·²å®Œæˆ: {len(existing)} ç¯‡\n")

    # è¿‡æ»¤æœªå®Œæˆçš„
    pending = [q for q in questions if q.get('slug') not in existing]
    print(f"âœ“ å¾…ç”Ÿæˆ: {len(pending)} ç¯‡\n")

    if not pending:
        print("ğŸ‰ æ‰€æœ‰æ–‡ç« å·²å®Œæˆï¼")
        return

    # æ‰¹é‡ç”Ÿæˆ (æ¯æ¬¡æœ€å¤š30ç¯‡)
    batch_size = 30
    pending_batch = pending[:batch_size]

    print(f"å¼€å§‹ç”Ÿæˆæ‰¹æ¬¡ 1 (æœ€å¤š {batch_size} ç¯‡)...\n")

    completed = 0
    failed = 0

    with ThreadPoolExecutor(max_workers=MAX_PARALLEL) as executor:
        futures = {}

        for q in pending_batch:
            slug = q.get('slug')
            question = q.get('question')
            volume = q.get('volume', 0)
            category = q.get('category_name', 'General')

            # RAG æ£€ç´¢
            context = rag.retrieve(question) if rag.loaded else ""

            future = executor.submit(
                generate_article,
                slug, question, volume, category, context
            )
            futures[future] = slug

        for future in as_completed(futures):
            slug = futures[future]
            try:
                success, result = future.result()
                if success:
                    completed += 1
                    total = len(existing) + completed
                    print(f"  [{total}/{len(questions)}] âœ“ {slug} - {result}")
                else:
                    failed += 1
                    print(f"  âœ— {slug}: {result}")
            except Exception as e:
                failed += 1
                print(f"  âœ— {slug}: {e}")

    print(f"\n" + "=" * 60)
    print(f" æœ¬æ‰¹æ¬¡å®Œæˆ:")
    print(f"   æ–°å¢: {completed} ç¯‡")
    print(f"   å¤±è´¥: {failed} ç¯‡")
    print(f"   æ€»è®¡: {len(get_existing_slugs())} ç¯‡")
    print("=" * 60)

    if len(pending) > batch_size:
        print(f"\nè¿˜æœ‰ {len(pending) - batch_size} ç¯‡å¾…ç”Ÿæˆï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬")


if __name__ == "__main__":
    main()
